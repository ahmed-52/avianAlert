{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poultry Disease Classification from Fecal Images\n",
    "\n",
    "This notebook classifies poultry diseases based on fecal images into four classes:\n",
    "- Healthy\n",
    "- Coccidiosis (cocci)\n",
    "- Salmonella (salmo)\n",
    "- Newcastle Disease (ncd)\n",
    "\n",
    "The model is optimized for Mac M2 with 8GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('Modules loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enable GPU Acceleration (for Mac M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac M2, enable Metal GPU acceleration\n",
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU acceleration enabled\")\n",
    "except:\n",
    "    print(\"No GPU found or GPU acceleration not available\")\n",
    "\n",
    "# Set smaller image size for better performance on 8GB RAM\n",
    "IMG_SIZE = (160, 160)  # Reduced from 224x224\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32  # Reduced from 40\n",
    "IMG_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Callback for Training Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience=1, stop_patience=3, threshold=0.9, factor=0.5, batches=None, epochs=None, ask_epoch=None):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self._model = model\n",
    "        self.patience = patience \n",
    "        self.stop_patience = stop_patience\n",
    "        self.threshold = threshold\n",
    "        self.factor = factor\n",
    "        self.batches = batches\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.count = 0\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1\n",
    "        \n",
    "        try:\n",
    "            self.current_lr = 0.001  # Default fallback\n",
    "            if hasattr(model.optimizer, 'learning_rate'):\n",
    "                lr = model.optimizer.learning_rate\n",
    "                if hasattr(lr, 'numpy'):\n",
    "                    self.current_lr = float(lr.numpy())\n",
    "            elif hasattr(model.optimizer, 'lr'):\n",
    "                lr = model.optimizer.lr\n",
    "                if hasattr(lr, 'numpy'):\n",
    "                    self.current_lr = float(lr.numpy())\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        self.initial_lr = self.current_lr\n",
    "        self.highest_tracc = 0.0\n",
    "        self.lowest_vloss = np.inf\n",
    "        self.best_weights = self._model.get_weights()\n",
    "        self.initial_weights = self._model.get_weights()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'Training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds'\n",
    "        print(msg)\n",
    "\n",
    "        self._model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end='')\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.ep_start = time.time()\n",
    "    \n",
    "\n",
    "    def _update_lr(self, new_lr):\n",
    "        \n",
    "        self.current_lr = new_lr\n",
    "        \n",
    "        # modern way to update learning rate - recreate optimizer\n",
    "        optimizer_config = self._model.optimizer.get_config()\n",
    "        if 'learning_rate' in optimizer_config:\n",
    "            optimizer_config['learning_rate'] = new_lr\n",
    "        elif 'lr' in optimizer_config:\n",
    "            optimizer_config['lr'] = new_lr\n",
    "            \n",
    "        optimizer_name = self._model.optimizer.__class__.__name__\n",
    "        new_optimizer = None\n",
    "        \n",
    "        # create the new optimizer with the updated learning rate\n",
    "        if optimizer_name == 'Adamax':\n",
    "            new_optimizer = tf.keras.optimizers.Adamax.from_config(optimizer_config)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            new_optimizer = tf.keras.optimizers.Adam.from_config(optimizer_config)\n",
    "        else:\n",
    "            # For other optimizers, try generic approach\n",
    "            new_optimizer = getattr(tf.keras.optimizers, optimizer_name).from_config(optimizer_config)\n",
    "            \n",
    "        # Compile with the new optimizer\n",
    "        self._model.compile(\n",
    "            optimizer=new_optimizer,\n",
    "            loss=self._model.loss,\n",
    "            metrics=self._model.metrics_names\n",
    "        )\n",
    "        \n",
    "        return new_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        # get current learning rate safely\n",
    "        current_lr = self.current_lr\n",
    "        acc = logs.get('accuracy')\n",
    "        v_acc = logs.get('val_accuracy')\n",
    "        loss = logs.get('loss')\n",
    "        v_loss = logs.get('val_loss')\n",
    "        \n",
    "        # default next learning rate\n",
    "        next_lr = current_lr\n",
    "\n",
    "        if acc < self.threshold:\n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc) * 100 / self.highest_tracc\n",
    "\n",
    "            if acc > self.highest_tracc:\n",
    "                self.highest_tracc = acc\n",
    "                self.best_weights = self._model.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    next_lr = current_lr * self.factor\n",
    "                    # update learning rate safely\n",
    "                    self._update_lr(next_lr)\n",
    "                    self.count = 0\n",
    "                    self.stop_count = self.stop_count + 1\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "        else:\n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss:\n",
    "                self.lowest_vloss = v_loss\n",
    "                self.best_weights = self._model.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    next_lr = current_lr * self.factor\n",
    "                    # update learning rate safely\n",
    "                    self._update_lr(next_lr)\n",
    "                    self.stop_count = self.stop_count + 1\n",
    "                    self.count = 0\n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{next_lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1:\n",
    "            msg = f'Training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(zip_dir, extract_dir):\n",
    "    \"\"\"\n",
    "    Extract all zip files containing the image data\n",
    "    \"\"\"\n",
    "    # create extract directory if it doesn't exist\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    # list of expected zip files\n",
    "    expected_zips = ['healthy.zip', 'cocci.zip', 'salmo.zip', 'ncd.zip']\n",
    "    \n",
    "    for zip_file in expected_zips:\n",
    "        zip_path = os.path.join(zip_dir, zip_file)\n",
    "        if os.path.exists(zip_path):\n",
    "            print(f\"Extracting {zip_file}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # extract to a subfolder named after the class (removing .zip extension)\n",
    "                class_name = zip_file.split('.')[0]\n",
    "                class_dir = os.path.join(extract_dir, class_name)\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                zip_ref.extractall(class_dir)\n",
    "            print(f\"Extracted {zip_file} to {class_dir}\")\n",
    "        else:\n",
    "            print(f\"Warning: {zip_file} not found in {zip_dir}\")\n",
    "\n",
    "def create_csv_from_directory(data_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    Create a CSV file with image paths and labels from directory structure\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through class directories\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Iterate through images in the class directory\n",
    "            for image_file in os.listdir(class_dir):\n",
    "                if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    # Use relative paths so it works in different environments\n",
    "                    image_path = os.path.join(class_name, image_file)\n",
    "                    filepaths.append(image_path)\n",
    "                    labels.append(class_name)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Created CSV file with {len(df)} images\")\n",
    "    return df\n",
    "\n",
    "def split_data(data_dir, csv_path):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = ['filepaths', 'labels']\n",
    "    else:\n",
    "        df = create_csv_from_directory(data_dir, csv_path)\n",
    "    \n",
    "    # Add full paths\n",
    "    df['filepaths'] = df['filepaths'].apply(lambda x: os.path.join(data_dir, x))\n",
    "    \n",
    "    # Create train df\n",
    "    strat = df['labels']\n",
    "    train_df, dummy_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    # Valid and test dataframe\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens(train_df, valid_df, test_df, batch_size, img_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Create image data generators for train, validation, and test sets\n",
    "    \"\"\"\n",
    "    channels = 3\n",
    "    color = 'rgb'\n",
    "    \n",
    "    # Calculate test batch size\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) \n",
    "                                  if ts_length % n == 0 and ts_length/n <= 80]))\n",
    "    \n",
    "    # Function for preprocessing\n",
    "    def scalar(img):\n",
    "        return img\n",
    "    \n",
    "    # Create generators with augmentation for training\n",
    "    tr_gen = ImageDataGenerator(\n",
    "        preprocessing_function=scalar,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        zoom_range=0.2\n",
    "    )\n",
    "    \n",
    "    ts_gen = ImageDataGenerator(preprocessing_function=scalar)\n",
    "    \n",
    "    # Flow from dataframes\n",
    "    train_gen = tr_gen.flow_from_dataframe(\n",
    "        train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    valid_gen = ts_gen.flow_from_dataframe(\n",
    "        valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    test_gen = ts_gen.flow_from_dataframe(\n",
    "        test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=False, batch_size=test_batch_size\n",
    "    )\n",
    "    \n",
    "    return train_gen, valid_gen, test_gen\n",
    "\n",
    "def show_images(gen):\n",
    "    \"\"\"\n",
    "    Show sample images from the generator\n",
    "    \"\"\"\n",
    "    g_dict = gen.class_indices\n",
    "    classes = list(g_dict.keys())\n",
    "    images, labels = next(gen)\n",
    "    \n",
    "    length = len(labels)\n",
    "    sample = min(length, 25)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])\n",
    "        class_name = classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    \n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    \n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label='Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, Without Normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment='center', \n",
    "                     color='white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Building and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(class_count, img_shape=(160, 160, 3)):\n",
    "    \"\"\"\n",
    "    Build the model (using EfficientNetB0 instead of B3 for better performance on Mac M2)\n",
    "    \"\"\"\n",
    "    # Create pre-trained model - using B0 which is smaller than B3\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=img_shape, \n",
    "        pooling='max'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001),\n",
    "        Dense(128, kernel_regularizer=regularizers.l2(0.016),  # Fixed: removed 'l=' parameter\n",
    "              activity_regularizer=regularizers.l1(0.006),\n",
    "              bias_regularizer=regularizers.l1(0.006), activation='relu'),\n",
    "        Dropout(rate=0.45, seed=123),\n",
    "        Dense(class_count, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        Adamax(learning_rate=0.001), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_gen, valid_gen, epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model with custom callback\n",
    "    \"\"\"\n",
    "    patience = 1\n",
    "    stop_patience = 3\n",
    "    threshold = 0.9\n",
    "    factor = 0.5\n",
    "    ask_epoch = 5\n",
    "    batch_size = BATCH_SIZE\n",
    "    batches = int(np.ceil(len(train_gen.labels) / batch_size))\n",
    "    \n",
    "    callbacks = [MyCallback(\n",
    "        model=model, patience=patience, stop_patience=stop_patience, \n",
    "        threshold=threshold, factor=factor, batches=batches, \n",
    "        epochs=epochs, ask_epoch=ask_epoch\n",
    "    )]\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=train_gen, epochs=epochs, verbose=0, callbacks=callbacks,\n",
    "        validation_data=valid_gen, validation_steps=None, shuffle=False\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_gen):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and display results\n",
    "    \"\"\"\n",
    "    # Reset generator to start\n",
    "    test_gen.reset()\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(test_gen, steps=len(test_gen), verbose=1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Get true classes\n",
    "    true_classes = test_gen.classes\n",
    "    class_labels = list(test_gen.class_indices.keys())\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plot_confusion_matrix(cm, class_labels, title='Confusion Matrix')\n",
    "    \n",
    "    # Print classification report\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)\n",
    "    print(f'\\nOverall Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preparation and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your directories here\n",
    "ZIP_DIR = \"chicken_feces_Zips\"  \n",
    "EXTRACT_DIR = \"extracted_images\"\n",
    "CSV_PATH = \"poultry_data.csv\"\n",
    "\n",
    "# Step 1: Extract zip files if needed\n",
    "print(\"Step 1: Extract zip files if needed\")\n",
    "if not os.path.exists(EXTRACT_DIR) or len(os.listdir(EXTRACT_DIR)) < 4:\n",
    "    extract_zip_files(ZIP_DIR, EXTRACT_DIR)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "ZIP_DIR = \"chicken_feces_Zips\"  \n",
    "EXTRACT_DIR = \"extracted_images\"\n",
    "CSV_PATH = \"poultry_data.csv\"\n",
    "\n",
    "\n",
    "print(\"\\nPrepare data\")\n",
    "def create_fixed_csv():\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Function to recursively find all image files in a directory and its subdirectories\n",
    "    def find_images_recursive(base_dir, class_name):\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    filepaths.append(file_path)\n",
    "                    labels.append(class_name)\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    # Process each class directory\n",
    "    total_count = 0\n",
    "    for class_name in ['healthy', 'cocci', 'salmo', 'ncd']:\n",
    "        class_dir = os.path.join(EXTRACT_DIR, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            print(f\"Processing {class_name} directory...\")\n",
    "            found = find_images_recursive(class_dir, class_name)\n",
    "            print(f\"  Found {found} images in {class_name}\")\n",
    "            total_count += found\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Created CSV file with {len(df)} images ({total_count} total)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "full_df = create_fixed_csv()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    # Split into train and temp dfs\n",
    "    strat = full_df['labels']\n",
    "    train_df, dummy_df = train_test_split(full_df, train_size=0.8, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    # Split temp into valid and test\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(valid_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    \n",
    "\n",
    "    print(\"\\nCreate generators\")\n",
    "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, BATCH_SIZE, img_size=IMG_SIZE)\n",
    "    \n",
    "    print(\"\\nBuilding the model\")\n",
    "    class_count = len(train_gen.class_indices)\n",
    "    model = build_model(class_count, img_shape=IMG_SHAPE)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    print(\"\\nTrain the model\")\n",
    "    EPOCHS = 20  # Set your desired epochs\n",
    "    history = train_model(model, train_gen, valid_gen, epochs=EPOCHS)\n",
    "\n",
    "\n",
    "    print(\"\\nPlot training history\")\n",
    "    plot_training(history)\n",
    "\n",
    "\n",
    "    print(\"\\nEvaluate model on test set\")\n",
    "    evaluate_model(model, test_gen)\n",
    "\n",
    "\n",
    "    print(\"\\n Save the model\")\n",
    "    model.save('poultry_disease_model.h5')\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"No images found! Make sure the zip files were properly extracted with image files inside.\")\n",
    "    print(\"Check the following directories:\")\n",
    "    for class_name in ['healthy', 'cocci', 'salmo', 'ncd']:\n",
    "        path = os.path.join(EXTRACT_DIR, class_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f\"  • {path} exists\")\n",
    "            # List some files in this directory\n",
    "            files = os.listdir(path)\n",
    "            print(f\"    - Contains {len(files)} files/directories\")\n",
    "            if files:\n",
    "                print(f\"    - Sample items: {files[:5]}\")\n",
    "        else:\n",
    "            print(f\"  • {path} does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
